{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406cc55c-edde-4e97-aad1-8ff30204e273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a803999dec5c4888bdb529be1d95af15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/340897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f2cf9ed8ae4c679bce65736ebf27b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c222e356262a4d7a8ab663bba1da85ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8939 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 340897\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8960\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8939\n",
      "    })\n",
      "})\n",
      "Tokenization completed for both datasets!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5638c3a63d4d189a800727d1e84ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/340897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed696e5e80b74914b9e2835a20451133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14516f503aa447aeb539e172fc432f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8939 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets saved.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"model_name\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/vinai-translate-en2vi-v2\", src_lang=\"en_XX\", tgt_lang=\"vi_VN\")\n",
    "\n",
    "def load_and_prepare_dataset(data_path, is_csv=False):\n",
    "    if is_csv:\n",
    "        # Load MIMIC-III dataset from CSV\n",
    "        dataset = load_dataset(\"csv\", data_files=data_path)[\"train\"]\n",
    "\n",
    "        # Split the dataset\n",
    "        full_split = dataset.train_test_split(test_size=0.1, seed=42)  # Separate 10% for test\n",
    "        train_val_split = full_split[\"train\"].train_test_split(test_size=0.2, seed=42)  # Split remaining 90% into 80% train, 20% validation\n",
    "\n",
    "        # Combine splits into a DatasetDict\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": train_val_split[\"train\"],\n",
    "            \"validation\": train_val_split[\"test\"],\n",
    "            \"test\": full_split[\"test\"]\n",
    "        })\n",
    "    else:\n",
    "        # Load MedEV dataset\n",
    "        dataset = load_dataset(data_path)\n",
    "        for split in dataset:\n",
    "            dataset[split] = dataset[split].shuffle(seed=42)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(dataset, dataset_name):\n",
    "    def preprocess_function(examples):\n",
    "        # Handling different datasets (MedEV vs MIMIC-III)\n",
    "        if dataset_name == \"MedEV\":\n",
    "            inputs = [item[\"text\"] for item in examples[\"en\"]]\n",
    "            targets = [item[\"text\"] for item in examples[\"vi\"]]\n",
    "        elif dataset_name == \"MIMIC-III Demo\":\n",
    "            inputs = examples[\"en\"]\n",
    "            targets = examples[\"vi\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset name: {dataset_name}\")\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            text_target=targets,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return model_inputs\n",
    "    \n",
    "    # Apply tokenization to dataset\n",
    "    tokenized_data = dataset.map(preprocess_function, batched=True)\n",
    "    # tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_data = tokenized_data.remove_columns([\"en\", \"vi\"])\n",
    "    return tokenized_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths for MedEV and MIMIC-III datasets\n",
    "    medev_path = \"Angelectronic/MedEV\"\n",
    "    mimic_path = \"./data/mimic-iii/MIMIC-III Demo.csv\" \n",
    "\n",
    "    # Load the MedEV dataset from disk (no need for CSV loading here)\n",
    "    medev_dataset = load_and_prepare_dataset(medev_path, is_csv=False)\n",
    "    \n",
    "    # Load the MIMIC-III dataset from CSV\n",
    "    # mimic_dataset = load_and_prepare_dataset(mimic_path, is_csv=True)\n",
    "\n",
    "    # Preprocess and tokenize each dataset\n",
    "    tokenized_medev = preprocess_data(medev_dataset, dataset_name=\"MedEV\")\n",
    "    print(tokenized_medev)\n",
    "    # tokenized_mimic = preprocess_data(mimic_dataset, dataset_name=\"MIMIC-III Demo\")\n",
    "\n",
    "    print(\"Tokenization completed for both datasets!\")\n",
    "\n",
    "    # Save tokenized datasets\n",
    "    tokenized_medev.save_to_disk(\"./tokenized_dataset/MedEV\")\n",
    "    # tokenized_mimic.save_to_disk(\"./tokenized_dataset/MIMIC-III\")\n",
    "    print(\"Tokenized datasets saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
